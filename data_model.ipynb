{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model =pd.read_excel(project_path +'/data/result/df_model_data.xlsx')\n",
    "if 'Unnamed: 0' in df_model.columns:\n",
    "    df_model = df_model.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_col = ['gender','保肝药']\n",
    "continuous_col=[x for x in df_model.columns if x not in discrete_col]\n",
    "continuous_col.remove('bmd_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 防止不同维特征数据差距过大，影响建模效果\n",
    "for i in continuous_col:\n",
    "    max_value = df_model[i].max()\n",
    "    df_model[i]=df_model[i].apply(lambda x: round(x/max_value,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bmd_label', '谷草转氨酶(干式)', 'MTX_tdm_48h', 'γ-谷氨酰酶(干式)', '红细胞分布宽度',\n",
       "       'gender', '谷丙转氨酶(干式)', 'RBC平均血红量', '红细胞', 'MTX_tdm_24h', '碱性磷酸酶(干式)',\n",
       "       '肌酐(干式)', '日剂量', 'RBC血红浓度', 'MTX_tdm_12h', '嗜碱性细胞百分比', 'RBC平均容量',\n",
       "       '总胆红素(干式)', '淋巴细胞绝对值', 'age', '嗜碱性细胞绝对值', '保肝药'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 插补数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林对缺失值进行插补\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def missing_value_interpolation(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    # 提取存在缺失值的列名\n",
    "    missing_list = []\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum() > 0:\n",
    "            missing_list.append(i)\n",
    "    missing_list_copy = missing_list.copy()\n",
    "    # 用该列未缺失的值训练随机森林，然后用训练好的rf预测缺失值\n",
    "    for i in range(len(missing_list)):\n",
    "        name=missing_list[0]\n",
    "        df_missing = df[missing_list_copy]\n",
    "        # 将其他列的缺失值用0表示。\n",
    "        missing_list.remove(name)\n",
    "        for j in missing_list:\n",
    "            df_missing[j]=df_missing[j].astype('str').apply(lambda x: 0 if x=='nan' else x)\n",
    "        df_missing_is = df_missing[df_missing[name].isnull()]\n",
    "        df_missing_not = df_missing[df_missing[name].notnull()]\n",
    "        y = df_missing_not[name]\n",
    "        x = df_missing_not.drop([name],axis=1)\n",
    "        # 列出参数列表\n",
    "        tree_grid_parameter = {'n_estimators': list((10, 50, 100, 150, 200))}\n",
    "        # 进行参数的搜索组合\n",
    "        grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_grid_parameter,cv=3)\n",
    "        #rfr=RandomForestRegressor(random_state=0,n_estimators=100,n_jobs=-1)\n",
    "        #根据已有数据去拟合随机森林模型\n",
    "        grid.fit(x, y)\n",
    "        rfr = RandomForestRegressor(n_estimators=grid.best_params_['n_estimators'])\n",
    "        rfr.fit(x, y)\n",
    "        #预测缺失值\n",
    "        predict = rfr.predict(df_missing_is.drop([name],axis=1))\n",
    "        #填补缺失值\n",
    "        df.loc[df[name].isnull(),name] = predict\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插补建模数据\n",
    "df_model_cb=missing_value_interpolation(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存插补数据\n",
    "writer = pd.ExcelWriter(project_path + '/data/result/df_model_data_插补.xlsx')\n",
    "df_model_cb.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关性检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------计算连续变量的spearmanr相关性系数---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  连续变量，spearmanr相关性检验(统计量r);\n",
    "print('--------------------------计算连续变量的spearmanr相关性系数---------------------------------')\n",
    "from scipy import stats\n",
    "t_list = []\n",
    "p_list = []\n",
    "q_list = []\n",
    "\n",
    "for i in continuous_col:\n",
    "    # 删除连续变量中的<、>号\n",
    "    df_model_cb[i] = df_model_cb[i].astype('str').apply(lambda x: re.sub(r'<|>', '',x))\n",
    "    x= df_model_cb[df_model_cb[i].astype('float').notnull()][i]\n",
    "    y= df_model_cb[df_model_cb[i].astype('float').notnull()]['bmd_label']\n",
    "    t, p = stats.spearmanr(x,y)\n",
    "    t = round(t, 2)\n",
    "    p = round(p, 3)\n",
    "    q = '斯皮尔曼'\n",
    "    # print(i, t, p)\n",
    "\n",
    "    t_list.append(t)\n",
    "    p_list.append(p)\n",
    "    q_list.append(q)\n",
    "df_spearmanr= pd.DataFrame(data={'连续检测指标': continuous_col,\n",
    "                                't值': t_list,\n",
    "                                'p值': p_list,\n",
    "                                '方法': q_list})\n",
    "df_spearmanr_1 = df_spearmanr[df_spearmanr['p值'] <= 0.05]\n",
    "df_spearmanr_2 = df_spearmanr[df_spearmanr['p值'] >= 0.05]  # 显著性不成立\n",
    "df_spearmanr = pd.concat([df_spearmanr_1,df_spearmanr_2], axis=0)\n",
    "\n",
    "df_spearmanr=df_spearmanr.sort_values(by=['p值'],ascending=False)\n",
    "df_spearmanr = df_spearmanr.reset_index(drop=True)\n",
    "\n",
    "writer = pd.ExcelWriter(project_path + '/data/result/df_temp_spearmanr相关性检测.xlsx')\n",
    "df_spearmanr.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_ml.utils_models import load_ml_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_ml import Predictor\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "x = df_model_cb.drop(['bmd_label'],axis=1)\n",
    "y = df_model_cb['bmd_label']\n",
    "\n",
    "tran_x, test_x, tran_y, test_y = train_test_split(x, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行过采样\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=0)\n",
    "\n",
    "tran_x_sm,tran_y_sm = sm.fit_resample(tran_x,tran_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       129\n",
      "           1       0.20      0.13      0.16        15\n",
      "\n",
      "    accuracy                           0.85       144\n",
      "   macro avg       0.55      0.54      0.54       144\n",
      "weighted avg       0.83      0.85      0.84       144\n",
      "\n",
      "[[121   8]\n",
      " [ 13   2]]\n"
     ]
    }
   ],
   "source": [
    "# 直接使用xgboost和catboost包，而不是auto_ml\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "from sklearn.metrics import r2_score,precision_score,classification_report,confusion_matrix\n",
    "# XGBoost模型\n",
    "xgb_model=xgboost.XGBClassifier(max_depth=5,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=500,\n",
    "                        min_child_weight=0.5,\n",
    "                        eta=0.1,\n",
    "                        gamma=0.5,\n",
    "                        reg_lambda=10,\n",
    "                        subsample=0.5,\n",
    "                        colsample_bytree=0.8,\n",
    "                        nthread=4,\n",
    "                        scale_pos_weight=1)\n",
    "\n",
    "# LightGBM模型\n",
    "# xgb_model=lightgbm.LGBMClassifier(iterations=300, learning_rate=0.2, loss_function='RMSE',random_state=3)\n",
    "# CatBoost模型\n",
    "# xgb_model=catboost.CatBoostClassifier(iterations=300, learning_rate=0.2, loss_function='RMSE',random_state=3)\n",
    "# xgb_model=xgboost.XGBClassifier('eta'=0.1,\n",
    "#                                 'gamma':0.1,\n",
    "#                                 'max-depth':0.5,\n",
    "#                                 'min_child_weighty':3,\n",
    "#                                 'objective':'binary:logistic'\n",
    "#                                )\n",
    "xgb_model.fit(tran_x_sm,tran_y_sm)\n",
    "predictions=xgb_model.predict(test_x)\n",
    "# print(predictions)\n",
    "print(classification_report(test_y, xgb_model.predict(test_x)))\n",
    "\n",
    "cm2_LogR_model = confusion_matrix(test_y, xgb_model.predict(test_x))\n",
    "print(cm2_LogR_model) #混肴矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林，GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 列出参数列表\n",
    "tree_grid_parameter = {'n_estimators': list((10, 50, 100, 150, 200))}\n",
    "# 进行参数的搜索组合\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "# grid = GridSearchCV(GradientBoostingClassifier(), param_grid=tree_grid_parameter, cv=3)\n",
    "# 根据已有数据去拟合随机森林模型\n",
    "grid.fit(tran_x, tran_y)\n",
    "rfr = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'])\n",
    "# rfr = GradientBoostingClassifier(n_estimators=grid.best_params_['n_estimators'])\n",
    "rfr.fit(tran_x, tran_y)\n",
    "# 预测缺失值\n",
    "predictions = rfr.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='linear', C=1.25)\n",
    "svr.fit(tran_x,tran_y)\n",
    "predictions=svr.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN训练\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(tran_x,tran_y)\n",
    "predictions=knn.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear回归，Lasso回归，领回归\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,LogisticRegression\n",
    "lcv = LogisticRegression()\n",
    "lcv.fit(tran_x, tran_y)\n",
    "predictions = lcv.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------计算R2和均方误差MSE---------------------------\n",
      "r2:  0.2\n",
      "MSE:  0.13333333333333333\n",
      "MAE:  0.16\n"
     ]
    }
   ],
   "source": [
    "# 计算R2和均方误差MSE\n",
    "print('-----------------------计算R2和均方误差MSE---------------------------')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error  # 均方误差\n",
    "from sklearn.metrics import mean_absolute_error  # 平方绝对误差\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score  # R square\n",
    "# 调用\n",
    "\n",
    "r2 = precision_score(test_y,predictions)\n",
    "print('r2: ',r2)\n",
    "mse=recall_score(test_y,predictions)\n",
    "print('MSE: ',mse)\n",
    "mae=f1_score(test_y,predictions)\n",
    "print('MAE: ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions= pd.DataFrame(data={'真实值':test_y,'预测值':predictions})\n",
    "writer = pd.ExcelWriter(project_path + '/data/result/df_model_测试集结果.xlsx')\n",
    "df_predictions.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断文件路径是否存在，如果不存在则创建该路径\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:  # 判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(path)  # makedirs 创建文件时如果路径不存在会创建这个路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "print('-----------------------画图---------------------------')\n",
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "# 散点图\n",
    "# axis设置坐标轴的范围\n",
    "# plt.axis([-20, 20, 0, 200])\n",
    "# x为x轴中坐标x的值，y为y轴中坐标y的值，x与y都是长度相同的数组序列，color为点的颜色，marker为散点的形状，\n",
    "# 折线图刻度调小，要不然点都堆到一块了\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_ylim(0,10)\n",
    "# plt.scatter(range(len(test_y)),test_y,c='r')\n",
    "plt.scatter(test_y,predictions,c='b')\n",
    "# 红色参照线\n",
    "plt.plot(list(range(test_y.shape[0])), list(range(test_y.shape[0])),color='r')\n",
    "# plt.plot(list(range(30)), list(range(30)),color='r')\n",
    "plt.xlabel('Number of Events(unit)')\n",
    "plt.ylabel('MTX Bone Suppression')\n",
    "# plt.show()\n",
    "# 判断图片保存路径是否存在，否则创建\n",
    "jpg_path = project_path + \"/jpg\"\n",
    "mkdir(jpg_path)\n",
    "plt.savefig(jpg_path + \"/他克莫司血药浓度测试集散点图v2.0.jpg\", dpi=300)\n",
    "plt.clf()  # 删除前面所画的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 重要性\n",
    "import catboost,xgboost\n",
    "model_boost=xgboost.XGBRegressor()\n",
    "model_boost.fit(tran_x,tran_y)\n",
    "importance = model_boost.feature_importances_\n",
    "print(tran_x.columns)\n",
    "print(importance)\n",
    "\n",
    "df_importance= pd.DataFrame(data={'特征':tran_x.columns,'重要性评分':importance})\n",
    "df_importance['重要性评分']=df_importance['重要性评分'].apply(lambda x: round(x,3))\n",
    "df_importance=df_importance.sort_values(['重要性评分'],ascending=False)\n",
    "df_importance=df_importance.reset_index(drop=True)\n",
    "writer = pd.ExcelWriter(project_path + '/result/df_19_模型重要性评分.xlsx')\n",
    "df_importance.to_excel(writer)\n",
    "writer.save()\n",
    "\n",
    "# SHAP图\n",
    "from pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  ##绘图显示中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "shap.initjs()  # notebook环境下，加载用于可视化的JS代码\n",
    "\n",
    "importance_list_10 = list(df_importance['特征'])[:10]\n",
    "tran_x=tran_x[importance_list_10]\n",
    "shap_model = xgb.XGBRegressor(max_depth=4, learning_rate=0.05, n_estimators=300)\n",
    "shap_model.fit(tran_x, tran_y)\n",
    "\n",
    "explainer = shap.TreeExplainer(shap_model)\n",
    "shap_values = explainer.shap_values(tran_x)  # 传入特征矩阵X，计算SHAP值\n",
    "# print(shap_values)\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, tran_x, plot_size=(20,12))\n",
    "# 保存各个变量的shape值的和\n",
    "df_shap_values=pd.DataFrame(shap_values)\n",
    "shap_list=[]\n",
    "for i in range(df_shap_values.shape[1]):\n",
    "    df_shap_values.iloc[:,i] = df_shap_values.iloc[:,i].apply(lambda x: abs(x))\n",
    "for j in range(df_shap_values.shape[1]):\n",
    "    feature_mean = df_shap_values.iloc[:,j].mean()\n",
    "    feature_mean = round(feature_mean, 2)\n",
    "    shap_list.append(feature_mean)\n",
    "\n",
    "df_shap = pd.DataFrame({'features':list(tran_x.columns),'shap值':shap_list})\n",
    "df_shap = df_shap.sort_values(by=['shap值'], ascending=False)\n",
    "df_shap = df_shap.reset_index(drop=True)\n",
    "\n",
    "writer = pd.ExcelWriter(project_path + '/result/df_20_shap值排序.xlsx')\n",
    "df_shap.to_excel(writer)\n",
    "writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "460.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
